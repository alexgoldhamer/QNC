PART ONE:

Exercise 1 

 

N=10 

P=0.2 

Q=0.8 

K=0-10 

 

Using the equations we were provided in the tutorial:  

K=0 --> (3628800)/(3628800)(.2)^0(.8)^10=0.107 

K=1 --> 0.2684 

K=2 --> 0.3020 

K=3 --> 0.2013 

K=4 --> 0.0881 

K=5 --> 0.0264 

K=6 --> 0.0055 

K=7 --> 0.00079 

K=8 --> 0.000074 

K=9 --> 0.0000041 

K=10 --> 0.0000001 

 

Exercise 2 

N=14 

P=0.1 or 0.7 or 0.1-1 in decile units  

Which seems most accurate given that 8 were released?  

Q=0.9 

 

What you get during experiment = release of 8 (K) 

 

The binomial coefficient is 8 choose 14, which equals 3003. We can just use the same equation as before but now vary "P" instead of "K". 

 

Probability of 8 quanta released given that the probability of each quantal release is 0.1-1:  

P=0.1 --> 3003(0.1)^8(.9)^6=1.6x10^-5 

P=0.2 --> 0.002 

P=0.3 --> 0.0232 

P=0.4 --> 0.0918 

P=0.5 --> 0.18 

P=0.6 --> 0.21 

P=0.7 --> 0.126 

P=0.8 --> 0.032 

P=0.9 --> 0.0013 

P=1 --> 0 

 

A 60% release probability is the most probable if we saw 8 quanta released since it yields the maximum likelihood.  

 

Exercise 3 

 

Repeat same thing as 8 quanta now with 5 quanta: 

P=0.1 --> 0.1216 

P=0.2 --> 0.1724 

P=0.3 --> 0.177 

P=0.4 --> 0.1367 

P=0.5 --> 0.0863 

P=0.6 --> 0.0452 

P=0.7 --> 0.0176 

P=0.8 --> 0.0049 

P=0.9 --> 0.0007 

P=1 --> 0 

 

Now multiply the values from quanta = 8 and quanta = 5: TOTAL LIKELIHOOD 

P=0.1 --> 1.95x10^-6 

P=0.2 --> 0.0003448 

P=0.3 --> 0.0041064 

P=0.4 --> 0.01254 

P=0.5 --> 0.015534 

P=0.6 --> 0.009492 

P=0.7 --> 0.0022176 

P=0.8 --> 0.0001568 

P=0.9 --> 9.1x10^-7 

P=1 --> 0 

 

The maximum value occurs at P=0.5.  

 

For the method that involves taking the log of each then adding them (which should be the same thing as taking the log of the product): LOG-LIKELIHOOD 

 

P=0.1 --> log(1.6x10^-5)+log(0.1216)=-13.15 

P=0.2 --> -7.97 

P=0.3 --> -5.49 

P=0.4 --> -4.38 

P=0.5 --> -4.17 

P=0.6 --> -4.66 

P=0.7 --> -6.11 
P=0.8 --> -8.76 

P=0.9 --> -13.91 

P=1 --> negative infinity  

 

Again, the maximum value occurs at P=0.5. 

 

Can you improve your estimate by computing the functions at a higher resolution:  

Yes, because let's say we only looked at P=0.1 and P=1 (for an extreme example), then we would have missed the value of "p" that maximizes the likelihood. Looking at high resolution (increments of 0.1) allowed us to identify this as occurring at P=0.5. 

 

How does the estimate improve as you increase the sample size: 

It shifts from estimating P=0.6 to P=0.5, the curve becomes tighter/narrower, the estimate gets better because there's more data points to draw from, more precise, less uncertainty  

 

Exercise 4 

Total number of quanta released: 

2x3+3x7+4x10....+10x5=653 

 

Total possible quanta released: 

100x14=1400 

 

Best estimate: 653/1400=0.466; on average, 46.6% of the available quanta are released per experiment.  

 

Exercise 5 

 

N (14) choose K (7) = 3432 

3432(0.3)^7(0.7)^7=0.0618 

 

Now repeating this for X=8-14: 

P(X=8): 0.0232 

P(X=9): 0.0066 

P(X=10): 0.0014 

P(X=11): 0.00022 

P(X=12): 0.0000223 

P(X=13): 0.000001 

P(X=14): 0.00000004 

 

Adding all these up I get: P(X greater than or equal to 7 | p=0.3) = 0.0932 

The probability that I would I gotten this measurement if my null hypothesis were true is roughly 9%. No, it would have to be less than 5%.  

Since p hat is the value that maximizes the likelihood of getting 7 out of 14, it should be 0.5. 


PART 2: https://www.nature.com/articles/s41598-024-68090-7

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from urllib.request import urlopen

# Load the dataset directly from UCI
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data"
column_names = ['age', 'operation_year', 'positive_axillary_nodes', 'survival_status']
data = pd.read_csv(url, header=None, names=column_names)

# Show first few rows
print(data.head())

# Binomial example: survival_status (1 = survived 5+ years, 2 = died within 5 years)
# We convert survival_status to 0 (survived) and 1 (died) for binomial trials
data['died'] = data['survival_status'] - 1

# Calculate probability of death within 5 years (p)
p_death = data['died'].mean()
print(f"Estimated probability of death within 5 years: {p_death:.3f}")

# Simulate Binomial data with n=1 (Bernoulli trials), p = estimated death probability
binomial_sim = np.random.binomial(n=1, p=p_death, size=len(data))

# Plot original vs simulated survival status
plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.hist(data['died'], bins=3, edgecolor='black')
plt.title('Original Survival Status (0=survived,1=died)')
plt.xlabel('Status')
plt.ylabel('Count')

plt.subplot(1,2,2)
plt.hist(binomial_sim, bins=3, edgecolor='black')
plt.title('Simulated Survival Status (Binomial)')
plt.xlabel('Status')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

# Poisson example: positive_axillary_nodes (count of positive lymph nodes)
# Calculate lambda (mean count)
lambda_poisson = data['positive_axillary_nodes'].mean()
print(f"Estimated mean positive axillary nodes (Î»): {lambda_poisson:.3f}")

# Simulate Poisson data with estimated lambda
poisson_sim = np.random.poisson(lam=lambda_poisson, size=len(data))

# Plot original vs simulated positive lymph nodes
plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.hist(data['positive_axillary_nodes'], bins=range(0, max(data['positive_axillary_nodes'])+2), edgecolor='black')
plt.title('Original Positive Axillary Nodes')
plt.xlabel('Number of nodes')
plt.ylabel('Count')

plt.subplot(1,2,2)
plt.hist(poisson_sim, bins=range(0, max(poisson_sim)+2), edgecolor='black')
plt.title('Simulated Poisson Data')
plt.xlabel('Number of nodes')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

 
